<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Chest X-Ray COVID-19 Classification</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<link rel="stylesheet" href="assets/css/prism.css">
		<script src="assets/js/prism.js"></script>

		<style>	
			/* Clear the float to ensure content after floats properly */
			.clearfix::after {
				content: "";
				clear: both;
				display: table;
			}
		</style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Projects</a>
					</header>

		<!-- Nav -->
		<nav id="nav">
			<ul class="links">
				<li class="active"><a href="index.html">Main Page</a></li>
				<li class="active"><a href="about.html">About Me</a></li>
				<li class="active"><a href="masters.html">Master's Thesis</a></li>
				<!--<li><a href="generic.html">Generic Page</a></li>
				<li><a href="elements.html">Elements Reference</a></li> -->
			</ul>
			<ul class="icons">
				<li><a href="https://www.linkedin.com/in/nathalia-kim/" target="_blank" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
				<li><a href="https://github.com/nathalia-kim" target="_blank" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
			</ul>
		</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<h1>Chest X-Ray COVID-19 Classification</h1>
									<p>Predict if a chest X-ray image is COVID-19 positive or negative using four different neural network architectures. (<b>Key concepts</b>: deep learning, computer vision, CNN, VGG16, ResNet, TensorFlow)</p>
								</header>
								<div class="image main"><img src="images//covid/cover.png" alt="" /></div>
								<h3>Problem Formulation</h3>
								<p>In this project, the goal is to predict if a <b>chest X-ray image is COVID-19 positive or negative</b> by building and training <b>neural network models</b>. The images were collected from different sources, were taken in different machines, and have different resolutions. The dataset is relatively small and therefore the results are less predictable. Nevertheless, the goal of this project is to have an <b>end-to-end learning framework</b>, running experiments with different model architectures and evaluating the results.  </p>
								<h3 >Exploratory Data Analysis</h3>
								<p style="margin: 0; padding: 0;">First let's <b>load</b> the data:</p>
									<pre><code  style="margin: 0; padding: 0;" class="language-python">from PIL import Image
import pandas as pd
import os
import numpy as np

def load_data(folder):
	images = []
	for file in os.listdir(folder):
		file_id = file.replace('.png', '')
		# read image, convert to 8-bit gray scale with alpha and resize
		image = Image.open(
			os.path.join(folder, file)
		).convert('LA').resize((256, 256))
		# get numpy array of image 
		arr = np.array(image)
		# add image id and array to list 
		images.append((int(file_id), arr))
	# sort list 
	images.sort(key=lambda i: i[0])
	return np.array([v for _id, v in images])

# load train images 
x_train = load_data(PARENT_PATH + 'train/train')
# read train infection labels 
y_train = pd.read_csv(PARENT_PATH + 'y_train.csv')['infection']
# load test images 
x_test = load_data(PARENT_PATH + 'test/test')</code></pre></p>

								<p style="margin: 0; padding: 0;">Next let's check the training set <b>dimensions</b>:</p>
								<pre ><code  style="margin: 0; padding: 0;" class="language-python">x_train.shape</code></pre>
								<code style="margin: 0; padding: 0;">(487, 256, 256, 2)</code>
								<p style="margin: 0; padding: 0;">That means we have 487 images in our training set, each image has dimensions 256 x 256 with 2 channels. Now let's <b>visualize</b> some images:</p>
								<pre ><code  style="margin: 0; padding: 0;" class="language-python">import matplotlib.pyplot as plt

# get 5 covid and 5 normal samples
covid_samples = (x_train[y_train==1][0:5])
normal_samples = (x_train[y_train==0][0:5])
samples = np.append(covid_samples, normal_samples, axis = 0)

fig, ax = plt.subplots(2,5, figsize=(25,10))
for i in range(10):
	img = samples[i]
	ax[i//5, i%5].imshow(img[:,:,0], cmap='gray')
	if i < 5:
		ax[i//5, i%5].set_title("COVID-19")
	else:
		ax[i//5, i%5].set_title("Normal")
	ax[i//5, i%5].axis('off')
	ax[i//5, i%5].set_aspect('auto')
plt.show()</code></pre>
							<div class="image main"><img src="images//covid/eda_viz.png" alt="" /></div>
							<p style="margin: 0; padding: 0;">And see the <b>distribution</b> of COVID-19 positive and negative in the training set:</p>
							<pre ><code  style="margin: 0; padding: 0;" class="language-python">print('Training size =', len(x_train))
print('Training labels size =', len(y_train))
print('Testing size =', len(x_test))
y_train.hist()</code></pre>
							<code>Training size = 487;
								Training labels size = 487;
								Testing size = 210</code>
							<div class="image left"><img src="images//covid/eda_plot.png" alt="" /></div>
							<div class="clearfix"></div>
							<p>So it seems we don't have any missing labels and there is almost double of COVID-19 positives than negatives in the training set. The testing set has 210 images, which is less than half of the training set. One of the challenges of this project certainly is the small size of the dataset.</p>

							<h2>Model Tuning</h2>
							<p >During model tuning, I experimented with <b>four different neural network architectures</b> to evaluate performance:</p>

							<ul>
								<li>Fully-connected neural network built from scratch</li>
								<li>Convolutional neural network (<b>CNN</b>) built from scratch</li>
								<li><b>Transfer learning</b> from a <b>VGG16</b> network pre-trained on ImageNet</li>
								<li><b>Transfer learning</b> from a <b>ResNet50</b> network pre-trained on ImageNet</li>
							</ul>

							<p>The neural networks <b>increase in complexity</b> as we move down the list. This was crutial to establish a baseline understanding of how both simple and complex models would perform on this problem, serving as a <b>proof-of-concept</b>. For each model, a wide range of <b>hyperparameters</b> were fine-tuned and several variances of the models were implemented, but here I will share a <b>summary</b> of the structure and optimization of the models.</p>

							<h3>Evaluation Metric</h3>
							<p style="margin: 0; padding: 0;">The evaluation metrics that were used for this project are <b>binary accuracy</b> and <b>AUROC</b> (area under receiver operating characteristic curve). <b>Binary accuracy</b> represents the frequency with which the predicted values match with actual values for binary labels. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) - percentage of false predictions that are true:</p>

							<div class="image left"><img src="images//covid/AUROC.png" alt="" /></div>
							<div class="clearfix"></div>

							<p>Therefore, the <b>ROC curve</b> serves as an effective means to visually assess the performance of a binary classifier. The <b>AUROC</b> quantifies the area under the ROC curve, providing a measure to evaluate the general performance of the model without setting a specific threshold and irrespective of the class distribution. </p>


							<h3>Fully-connected Neural Network</h3>
							<p>The first model I experimented with was a <b>simple</b>, fully-connected neural network built from <b>scratch</b>. The model has an initial layer to <b>flatten</b> the input image to an 1D array, then a couple of <b>dense layers</b> with 64 and 32 nodes each followed by <b>dropout layers</b> for <b>regularization</b>. The last layer is composed of a single node used for prediction of the output using the <b>Sigmoid</b> as the <b>activation function</b>:</p>
							<pre ><code  style="margin: 0; padding: 0;" class="language-python">import tensorflow as tf
from tensorflow.keras.layers import Flatten, Dense, Dropout, Input
 
def build():
	# specify input shape. Output shape: [(None, 256, 256, 2)] 
	img_in = Input(shape=(256, 256, 2))

	# flatten image to 1D array. Output shape: (None, 131072)
	flattened = Flatten()(img_in)

	# define first layer, 64 nodes. Output shape: (None, 64)
	fc1 = Dense(64)(flattened)

	# applies dropout to the input at input rate. Output shape: (None, 64)
	fc1 = Dropout(0.3)(fc1)

	# define second layer, 32 nodes. Output shape: (None, 32)
	fc2 = Dense(32)(fc1)

	# applies dropout to the input at input rate. Output shape: (None, 32)
	fc2 = Dropout(0.3)(fc2)

	# last layer, single node for prediction, use sigmoid as activation function 
	# output shape: (None, 1)
	output = Dense(1, activation = 'sigmoid')(fc2)

	model = tf.keras.Model(inputs=img_in, outputs=output)
	return model
 
model = build()

# use Adam optimizer and binary cross entropy as objective function 
# binary accuracy and AUC as metrics to be evaluated by the model
model.compile(
		optimizer=tf.keras.optimizers.Adam(),
		loss='binary_crossentropy',
		metrics=['BinaryAccuracy', 'AUC']
		)

model.summary() </code></pre>
							<div style="width: 450px; height: auto;"><img src="images/covid/fcnn.png" alt="" style="width: 100%; height: auto;" /></div>
							<div class="clearfix"></div>
							<p style="margin: 0; padding: 0;" >Now we can <b>train the model</b> for the number of <b>epochs</b> defined below. The <b>batch size</b> is related to the number of samples per gradient update and the <b>validation split</b> defines the fraction of the training data to be used as validation data:</p>
							<pre><code  style="margin: 0; padding: 0;" class="language-python"># define number of epochs and batch size 
epochs = 100
batch_size = 64

# x_train is the input data, y_train is the target data 
adam_history = model.fit(x = x_train, y = y_train,
					batch_size = batch_size, validation_split=0.3, epochs=epochs)

plot_performance(history)

# use the trained model to do a prediction on test data 
y_test = model.predict(x_test)</code></pre>

							<div class="image fit"><img src="images//covid/fcnn_plot_performance.png" alt="" /></div>

							<p>From the performance plots, we can see that both <b>accuracy</b> and <b>AUC</b> metrics exhibited substantial <b>oscillations</b> throughout the training process, reaching approximately 0.7 for both. This model got an AUC score of 0.62 on the test data, which is not very optimal. In practice, a <b>fully-connected neural network model isn't the best suited for processing imaging data</b> due to its need for a large number of parameters to interpret imaging features. This model is susceptible to <b>overfitting</b> and doesn't effectively address the <b>position invariance problem</b> associated with imaging features. To tackle these issues, a more suitable model would be a <b>convolutional neural network</b>, and that's the model I'll be experimenting with next. </p>

							<h3>Convolutional Neural Network (CNN)</h3>
							<p>As previously mentioned, the next model I will experiment with is a <b>convolutional neural network</b> built from scratch. A CNN is specifically designed for processing grid-like data, such as <b>images</b>, by using <b>kernels</b> (also referred to as filters or scanners) to extract features, making it more effective in <b>capturing spatial hierarchies</b> compared to a fully-connected neural network. In a CNN, kernels are applied to an input image through <b>convolution operations</b>, extracting features by systematically scanning the image, and producing an <b>output feature map</b> that highlights learned patterns and structures. </p>
							<p>The model is composed of <b>three convolutional blocks</b>, each initiating with a <b>2D convolutional layer</b> with 64, 128, and 256 filters, followed by a <b>max-pooling</b> layer (for output aggregation), <b>batch normalization</b>, and <b>dropout</b> for regularization in each block. The convolutional layers use ReLU as activation function and zero padding to generate the same output shape. The final layers are composed of a <b>hidden layer</b> with 512 nodes, followed by batch normalization, dropout and a <b>final node for prediction</b>, using <b>sigmoid</b> as activation function. 

							</p>

							<pre ><code   class="language-python">from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization

def build_cnn():
img_in = Input(shape=(256, 256, 2))

# 2D convolution layer with 64 filters that are 3x3 
# output shape: (None, 256, 256, 64)
conv1 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(img_in)
# pooling layer using max function within a 2x2 window 
# output shape: (None, 128, 128, 64) 
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
# normalization layer - output shape: (None, 128, 128, 64) 
norm1 = BatchNormalization(axis = -1)(pool1)
# applies dropout to the input at input rate 
# output shape: (None, 128, 128, 64)
drop1 = Dropout(rate=0.2)(norm1)

# 2D convolution layer with 128 filters that are 3x3 
# output shape: (None, 128, 128, 128)
conv2 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(drop1)
# pooling layer using max function within a 2x2 window 
# output shape: (None, 64, 64, 128)                             
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
# normalization layer - output shape: (None, 64, 64, 128)
norm2 = BatchNormalization(axis = -1)(pool2)
# applies dropout to the input - output shape: (None, 64, 64, 128) 
drop2 = Dropout(rate=0.2)(norm2)

# 2D convolution layer with 256 filters that are 3x3 
# output shape: (None, 64, 64, 256)
conv3 = Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same')(drop2)
# pooling layer using max function within a 2x2 window  
# output shape: (None, 32, 32, 256)                              
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
# normalization layer - output shape: (None, 32, 32, 256)
norm3 = BatchNormalization(axis = -1)(pool3)
# applies dropout to the input at input rate 
# output shape: (None, 32, 32, 256)
drop3 = Dropout(rate=0.2)(norm3)

# flatten image to 1D array - output shape: (None, 262144)
flat = Flatten()(drop3)  

# dense layer with 512 nodes, uses ReLU as activation function 
# output shape: (None, 512)
hidden1 = Dense(512, activation='relu')(flat)
# layer that normalizes its inputs - output shape: (None, 512)
norm4 = BatchNormalization(axis = -1)(hidden1)
# applies dropout to the input at input rate - output shape: (None, 512)
drop4 = Dropout(rate=0.2)(norm4)

# single node for prediction, sigmoid as activation function 
# output shape: (None, 1)
output = Dense(1, activation='sigmoid')(drop4)  

cnn = tf.keras.Model(inputs=img_in, outputs=output)

return cnn</code></pre>

							<p style="margin: 0; padding: 0;">Due to the small size of the dataset, I will perform <b>data augmentation</b> to improve generalizability of the model and implement <b>k-fold cross validation</b> as the evaluation technique:</p>

							<pre ><code  class="language-python">from sklearn.model_selection import train_test_split, StratifiedKFold

# get folds from training data 
folds = list(StratifiedKFold(n_splits=10, shuffle=True, 
			random_state=1).split(x_train, y_train))

# specify image generator - data augmentation
gen = ImageDataGenerator(horizontal_flip = True, vertical_flip = True,
						width_shift_range = 0.1, height_shift_range = 0.1,
						zoom_range = 0.1, rotation_range = 10)</code></pre>

						<p style="margin: 0; padding: 0;">Now we can finish compiling and training the model:</p>
						<pre><code class="language-python">epochs = 30
batch_size = 20

cnn = build_cnn()

# use Adam optimizer and binary cross entropy as objective function 
cnn.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy',
	metrics=['BinaryAccuracy', 'AUC'])

# train for each fold 
for j, (train_idx, val_idx) in enumerate(folds):

	# get split data for each fold
	print('\nFold ',j)
	X_train_cv = x_train[train_idx]
	y_train_cv = y_train[train_idx]
	X_valid_cv = x_train[val_idx]
	y_valid_cv= y_train[val_idx]

	# data augmentation
	generator = gen.flow(X_train_cv, y_train_cv, batch_size = batch_size)

	history = cnn.fit(generator, steps_per_epoch=len(X_train_cv)/batch_size,
				epochs=epochs, shuffle=True, verbose=1,
				validation_data = (X_valid_cv, y_valid_cv))
</code></pre>
							<div style="width: 450px; height: auto;"><img src="images/covid/cnn_plot.png" alt="" style="width: 100%; height: auto;" /></div>
							<div class="clearfix"></div>

							<p>The performance plots indicate improvement compared to the fully-connected neural network, with the <b>accuracy</b> and <b>AUC</b> reaching levels of 0.85 and 0.9 on the training data, respectively. On the testing data, the model achieved an <b>AUC score</b> of 0.72, representing an <b>improvement over the previous model</b>.</p>

							<h3>VGG16 network pre-trained on ImageNet</h3>
							<p style="margin: 0; padding: 0;">Next, I will employ a technique called <b>transfer learning</b> for training the model. Transfer learning is a machine learning technique which involves taking a pre-trained model from a prior task and fine-tuning it to enhance its generalization capabilities for a different task. To leverage this technique, I explored the <b>VGG16 network</b> (shown below) initially trained on the <b>Imagenet</b> dataset. The model was implemented using a transfer learning approach by loading the neural network and making modifications to specific architecture components and retraining them so the model can generalize to this problem and dataset.</p>

							<div style="width: 85%; height: auto;"><img src="images/vgg16.png" style="width: 85%; height: auto;" alt="" /></div>

							<p style="margin: 0; padding: 0;">First, we need to read the images as RGB:</p>

							<pre><code class="language-python">def load_rgb(folder):
# create images empty list
images = []
# loop through every image in folder
for file in os.listdir(folder):
	# get image id by taking filename
	file_id = file.replace('.png', '')
	# read image, convert to 8-bit gray scale with alpha and resize
	image = Image.open(
		os.path.join(folder, file)
	).convert('RGB').resize((256, 256))
	# get numpy array of image
	arr = np.array(image)
	# add image id and array to list
	images.append(
		(int(file_id), arr)
	)
# sort list
images.sort(key=lambda i: i[0])
return np.array([v for _id, v in images])

# load train images
x_train = load_rgb(PARENT_PATH + 'train/train')
# read train infection labels
y_train = pd.read_csv(PARENT_PATH + 'y_train.csv')['infection']
# load test images
x_test = load_rgb(PARENT_PATH + 'test/test')</code></pre>

								<p style="margin: 0; padding: 0;">As a next step, the model was loaded and modified for <b>fine-tuning</b>. To help with generalization, additional parameters were added for <b>data augmentation</b>. The <b>pre-trained VGG16 model</b> was loaded without the head of the model so we could add our own customized model head. The <b>model head</b> was constructed using an average pooling layer, followed by a flatten layer to get a 1D output, a dense layer with 64 nodes, a dropout layer to reduce <b>overfitting</b>, and a final dense layer with <b>softmax</b> activation function. </p>

								<pre><code class="language-python">from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.preprocessing.image import ImageDataGenerator

learning_rate = 1e-3
epochs = 100
batch_size = 20

# added parameters to the data augmentation object
trainAug = ImageDataGenerator(
		rotation_range=20,
		zoom_range=0.15,
		width_shift_range=0.2,
		height_shift_range=0.2,
		shear_range=0.15,
		horizontal_flip=True,
		fill_mode="nearest")

# one-hot encoding on the y_train
lb = LabelBinarizer()
labels = lb.fit_transform(y_train)
labels = to_categorical(labels)

# training (80 percent) and testing (20 percent) splits
(trainX, testX, trainY, testY) = train_test_split(x_train, labels,
	test_size=0.20, stratify=labels, random_state=42)

# load the VGG16 network, set FC layer to off
# output shape: [(None, 256, 256, 3)]
baseModel = VGG16(weights="imagenet", include_top=False,
	input_tensor=Input(shape=(256, 256, 3)))

# construct the head of the model that will be placed on top of the
# the base model
headModel = baseModel.output
# pooling layer with average function
# output shape: (None, 2, 2, 512)
headModel = AveragePooling2D(pool_size=(4, 4))(headModel)
# flatten layer to get 1D output
# output shape: (None, 2048)
headModel = Flatten(name="flatten")(headModel)
# dense layer with 64 nodes
# output shape: (None, 64)
headModel = Dense(64, activation="relu")(headModel)
# dropout layer to reduce overfitting
# output shape: (None, 64)
headModel = Dropout(0.5)(headModel)
# dense layer with 2 nodes
# output shape: (None, 2)
headModel = Dense(2, activation="softmax")(headModel)

# place the head FC model on top of the base model
# now we have our final model
model = Model(inputs=baseModel.input, outputs=headModel)

# loop over all layers in the base model and freeze them so they will
# not be updated during the first training process
for layer in baseModel.layers:
	layer.trainable = False

# compile the model
print("[INFO] compiling model...")
# define adam otimizer
opt = Adam(lr = learning_rate, decay= learning_rate / epochs)
# compile model with optimizer and evaluate given metrics
# we are using binary accuracy and AUC as metrics
model.compile(loss="binary_crossentropy", optimizer=opt,
	metrics=['BinaryAccuracy', 'AUC'])

# train the head of the model
# use augmented data
print("[INFO] training head...")
history = model.fit_generator(
	trainAug.flow(trainX, trainY, batch_size=batch_size),
	steps_per_epoch = len(trainX) // batch_size,
	validation_data =(testX, testY),
	validation_steps =len(testX) // batch_size,
	epochs = epochs)</code></pre>

							<div class="image fit"><img src="images/covid/vgg16_plot.png" alt="" /></div>

							<p>The <b>final VGG16 model</b> achieved an <b>accuracy</b> of 0.75 on the validation set and 0.83 on the training set. In terms of <b>AUC</b>, the model achieved 0.81 on the <b>validation data</b> and AUC of 0.92 on the <b>training data</b>. On the <b>testing set</b>, the model showed an AUC score of 0.81, which is a <b>good improvement compared to the previous CNN model</b>.</p>

							<h3>Resnet50 network pre-trained on ImageNet</h3>
							<p>The last model that was implemented was the <b>ResNet50</b>, also following a <b>transfer learning </b>approach by loading a model pre-trained on the <b>Imagenet</b>. The ResNet (residual network) is a convolutional neural network with added residual connections, allowing the network to have more convolutional layers while <b>avoiding the vanishing gradient problem</b>. Similarly to the VGG16 model, the pretrained ResNet50 model was loaded and a <b>customized model head</b> was built and connected to the model body: </p>

							<pre><code class="language-python">from tensorflow.keras.applications import ResNet50

batch_size = 10
epochs = 100
learning_rate = 1e-5

# one-hot encoding on y_train
lb = LabelBinarizer()
labels = lb.fit_transform(y_train)
labels = to_categorical(labels)

# training (80 percent) and testing (20 percent) splits
(trainX, testX, trainY, testY) = train_test_split(x_train, labels,
	test_size=0.20, stratify=labels, random_state=42)

# create data augmentation object
# list of parameters to apply data augmentation
trainAug = ImageDataGenerator(
	rotation_range=15,
	zoom_range=0.1,
	width_shift_range=0.1,
	height_shift_range=0.1,
	shear_range=0.2,
	horizontal_flip=True,
	fill_mode="nearest")

# define mean subtraction
# set the mean subtraction value to trainAug object
mean = np.array([123.68, 116.779, 103.939], dtype="float32")
trainAug.mean = mean

# load the ResNet network, let the head FC layer off
# output shape: (None, 256, 256, 3)
print("[INFO] preparing model...")
baseModel = ResNet50(weights="imagenet", include_top=False,
	input_tensor=Input(shape=(256, 256, 3)))

# construct the head of the model
# we will place the head on top of the model afterwards
headModel = baseModel.output

# pooling layer with average function
# output shape: (None, 1, 1, 2048)
headModel = AveragePooling2D(pool_size=(7, 7))(headModel)
# flatten layer to get 1D array
# output shape: (None, 2048)
headModel = Flatten(name="flatten")(headModel)
# dense layer using ReLU as activation function
# output shape: (None, 256)
headModel = Dense(256, activation="relu")(headModel)
# dropout layer as regularization
# output shape: (None, 256)
headModel = Dropout(0.5)(headModel)
# dense layer with softmas activation function
# output shape: (None, 2)
headModel = Dense(2, activation="softmax")(headModel)

# place the head FC model on top of the base model
model = Model(inputs=baseModel.input, outputs=headModel)

# loop over all layers in the base model and freeze them so they will
# not be updated during the training process
for layer in baseModel.layers:
	layer.trainable = False

# use adam as optimization algorithm
opt = Adam(lr= learning_rate, decay= learning_rate / epochs)
# compile the model
# we are using binary accuracy and AUC as evaluation metrics
model.compile(loss="binary_crossentropy", optimizer=opt,
	metrics=['BinaryAccuracy', 'AUC'])

# train the model
print("[INFO] training model...")
history = model.fit(
	trainAug.flow(trainX, trainY, batch_size = batch_size),
	steps_per_epoch= len(trainX) // batch_size,
	validation_data=(testX, testY),
	validation_steps= len(testX) // batch_size,
	epochs=epochs)</code></pre>
							
							<div class="image fit"><img src="images/covid/resnet_plot.png" alt="" /></div>

							<p>The <b>final ResNet50 model</b> achieved an <b>accuracy</b> of 0.77 on the validation set and 0.81 on the training set. In terms of <b>AUC</b>, the model achieved 0.85 on the <b>validation data</b> and AUC of 0.91 on the <b>training data</b>. On the <b>testing set</b>, the model showed an AUC score of 0.87, which shows an <b> improvement compared to the previous VGG16 model</b>.</p>

							<h3>Conclusions</h3>
							<p>In this project, I built an <b>end-to-end learning framework</b> to predict if a <b>chest X-ray image is COVID-19 positive or negative</b>. I gained a better understanding of the data through <b>exploratory data analysis</b> and built <b>four different neural network architectures</b>, fine-tuning several parameters and evaluating the results. The <b>overall AUC score</b> of each model on <b>unseen, testing data</b> was:</p>

							<ul>
								<li>Fully-connected neural network built from scratch: <b>0.62</b></li>
								<li>Convolutional neural network (<b>CNN</b>) built from scratch: <b>0.72</b></li>
								<li><b>VGG16</b> network pre-trained on ImageNet: <b>0.81</b></li>
								<li><b>ResNet50</b> network pre-trained on ImageNet: <b>0.87</b></li>
							</ul>

							<p>The <b>top performing model was the ResNet50</b> network with an AUC of 0.87. It is interesting to note that the <b>complexity of the models increase</b> as we go down the list above, as well as the final model performance. Indeed, a <b>fully-connected neural network</b> is not the most ideal architecture to work with image data as discussed previously, while the <b>ResNet50 architecture provided a considerable innovation</b> in computer vision, allowing the use of residual learning and the skipping of connections.</p>

							<p>Another distinguishing factor of the last two models was the use of the <b>transfer learning technique</b>, leveraging models pretrained on another dataset and <b>fine-tuning</b> them for this data. One notable <b>limitation of this project</b> is the relatively small size of the dataset, which might contribute to the models not achieving a higher performance. Nevertheless, in this project, I designed models from scratch and fine-tuned pre-trained models with increasing levels of complexity. <b>I used a number of techniques in deep learning to successfully build a full pipeline for classification of COVID-19 image data</b>.</p>

							</section>

					</div>

				<!-- Footer -->
				<footer id="footer">
					<section class="split contact">
						<section>
							<h3>Email</h3>
							<p><a>nathalia.yun@gmail.com</a></p>
						</section>
						<section>
							<h3>Social</h3>
							<ul class="icons alt">
								<li><a href="https://www.linkedin.com/in/nathalia-kim/" target="_blank" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://github.com/nathalia-kim" target="_blank" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
					</section>
				</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>